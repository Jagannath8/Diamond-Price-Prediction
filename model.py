# -*- coding: utf-8 -*-
"""DiamondPricePrediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11wfbzEkPrOhIiSqu0CXiCpJszNQBy24p

# **Diamond Price Prediction**

# Importing Libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn

"""# Reading the Data"""

df = pd.read_csv('/content/diamonds.csv')
df

"""# Understanding the Data"""

df.dtypes

df.shape

df.size

df.columns

df.info()

df.describe()

df.corr()

df.nunique()

df.isnull().any()

# from sklearn.preprocessing import LabelEncoder
# labelencoding = LabelEncoder()
# categories=['cut', 'color',	'clarity']
# df[categories]=df[categories].apply(lambda x:labelencoding.fit_transform(x))

"""# One Hot Encoding"""

final_df = pd.get_dummies(df, drop_first=True)
final_df

"""# Visualiztion"""

import missingno as no
no.bar(df, color='lightblue')

sns.heatmap(df.isnull(), yticklabels='False', cmap='Greens')

df1 = df['cut'].value_counts()
plt.pie(df1.values, labels=df1.index, autopct='%0.2f%%')
plt.title('Pecentage of Cut', fontsize=15)
plt.show()

sns.distplot(df['price'], color='red')

plt.hist(df['carat'],bins = 30, color='g')
plt.xlabel('carat')
plt.ylabel('price')
plt.show()

plt.figure(figsize=(14,8))
sns.violinplot(x=df.color, y=df.carat, palette='rainbow')

plt.figure(figsize=(10,5))
sns.stripplot(x=df['price'], y=df.clarity, palette='magma_r')
plt.show()

sns.boxplot(x='clarity',y='price',data=df)
plt.show()

sns.pairplot(df)

plt.figure(figsize=(22,20))
sns.heatmap(final_df.corr(), yticklabels='auto', annot=True)
plt.show()

"""# Splitting the Data into Dependent and Independent Variables"""

x = final_df.drop(['price'], axis=1)
y = final_df["price"]

x.shape

"""# Feature Importance"""

from sklearn.ensemble import ExtraTreesRegressor
model = ExtraTreesRegressor()
model.fit(x,y)
print(model.feature_importances_)

feat_imp = pd.Series(model.feature_importances_, index=x.columns)

feat_imp.nlargest(5).plot(kind='barh')

"""# Training and Testing the Data"""

from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.3, random_state=20)

"""# Linear Regression"""

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(xtrain, ytrain)

"""## Prediction"""

ypred_train = lr.predict(xtrain)
ypred_test = lr.predict(xtest)

"""## Accuracy"""

from sklearn import metrics
print("Accuracy of training data:", metrics.r2_score(ytrain, ypred_train)*100)
ac1 = metrics.r2_score(ytest, ypred_test)*100
print("Accuracy of testing data:", ac1)

"""## Error"""

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(ytest, ypred_test)
print("Mean Square Error:", mse)

"""# Random Forest Regressor"""

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()

rf.fit(xtrain, ytrain)

"""## Prediction"""

ypred_train = rf.predict(xtrain)
ypred_test = rf.predict(xtest)

"""## Accuracy"""

from sklearn import metrics
print("Accuracy of training data:", metrics.r2_score(ytrain, ypred_train)*100)
ac2 = metrics.r2_score(ytest, ypred_test)*100
print("Accuracy of testing data:", ac2)

"""## Error"""

from sklearn.metrics import mean_squared_error
mse = mean_squared_error(ytest, ypred_test)
print("Mean Square Error:", mse)

"""# Comparing Accuracy of Different Models"""

accuracy =  {ac1: 'Linear Regression', ac2: 'Random Forest Regressor'}

sns.set_style('darkgrid')
plt.figure(figsize=(10, 8))
model_accuracies = list(accuracy.values())
model_names = list(accuracy.keys())
sns.barplot(x=model_accuracies, y=model_names, palette='gist_rainbow')

"""Random Forest Regressor has more accuracy ie 97.42%

Hence we will save this model

# Saving the Model
"""

import pickle
file = open('diamond_price_model.pkl', 'wb')
pickle.dump(rf, file)